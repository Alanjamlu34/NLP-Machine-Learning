{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMJL2WMItu6TVz0rxy9TQS6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alanjamlu34/NLP-Machine-Learning/blob/main/IndoNLU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library and Dataset"
      ],
      "metadata": {
        "id": "odTwHNPdvSSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/indobenchmark/indonlu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjBbN41euLJo",
        "outputId": "cf28f12f-8f00-4284-d188-143bcb72801e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'indonlu'...\n",
            "remote: Enumerating objects: 500, done.\u001b[K\n",
            "remote: Counting objects: 100% (184/184), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 500 (delta 115), reused 139 (delta 110), pack-reused 316\u001b[K\n",
            "Receiving objects: 100% (500/500), 9.45 MiB | 24.63 MiB/s, done.\n",
            "Resolving deltas: 100% (235/235), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "from indonlu.utils.forward_fn import forward_sequence_classification\n",
        "from indonlu.utils.metrics import document_sentiment_metrics_fn\n",
        "from indonlu.utils.data_utils import DocumentSentimentDataset, DocumentSentimentDataLoader"
      ],
      "metadata": {
        "id": "VN99sd5quMIw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "# common functions\n",
        "###\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "def count_param(module, trainable=False):\n",
        "    if trainable:\n",
        "        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
        "    else:\n",
        "        return sum(p.numel() for p in module.parameters())\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def metrics_to_string(metric_dict):\n",
        "    string_list = []\n",
        "    for key, value in metric_dict.items():\n",
        "        string_list.append('{}:{:.2f}'.format(key, value))\n",
        "    return ' '.join(string_list)"
      ],
      "metadata": {
        "id": "dGnRPizFuOt4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(19072021)"
      ],
      "metadata": {
        "id": "L_LuquUJu3en"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Konfigurasi dan Load Pre-trained Model"
      ],
      "metadata": {
        "id": "AujJ7ROovM-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Tokenizer and Config\n",
        "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
        "config = BertConfig.from_pretrained('indobenchmark/indobert-base-p1')\n",
        "config.num_labels = DocumentSentimentDataset.NUM_LABELS\n",
        "\n",
        "# Instantiate model\n",
        "model = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p1', config=config)"
      ],
      "metadata": {
        "id": "EOz9Ei8jvF6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh0kz89-vPAZ",
        "outputId": "0ed5347e-74dd-4350-d320-59faba52b8e9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(50000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Persiapan Dataset Analisis Sentimen"
      ],
      "metadata": {
        "id": "RpqkUDtDvmzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_path = '/content/indonlu/dataset/smsa_doc-sentiment-prosa/train_preprocess.tsv'\n",
        "valid_dataset_path = '/content/indonlu/dataset/smsa_doc-sentiment-prosa/valid_preprocess.tsv'\n",
        "test_dataset_path = '/content/indonlu/dataset/smsa_doc-sentiment-prosa/test_preprocess_masked_label.tsv'"
      ],
      "metadata": {
        "id": "lPsRGLaMvk76"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = DocumentSentimentDataset(train_dataset_path, tokenizer, lowercase=True)\n",
        "valid_dataset = DocumentSentimentDataset(valid_dataset_path, tokenizer, lowercase=True)\n",
        "test_dataset = DocumentSentimentDataset(test_dataset_path, tokenizer, lowercase=True)\n",
        "\n",
        "train_loader = DocumentSentimentDataLoader(dataset=train_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=True)\n",
        "valid_loader = DocumentSentimentDataLoader(dataset=valid_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=False)\n",
        "test_loader = DocumentSentimentDataLoader(dataset=test_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=False)"
      ],
      "metadata": {
        "id": "fsnC7ppwvpuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5W8IkJ91wroa",
        "outputId": "a78a92f8-29d7-4841-ffb1-41e59fe785a6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([    2,  6540,    92,  2970,   213,  4259,  3553,   899,    34,\n",
            "         259,  5590,   262,  2558,   386,   899,  1687,    26,  1574,\n",
            "       30470,   899,  3310, 30468, 22130, 30360,  6123,  6368, 30468,\n",
            "       22130, 30360,  2652,  1746, 30468,  8869,  6540,    34,  6315,\n",
            "        1622,  1256,  8949,   899, 30468,  4222,  1622,   752,   245,\n",
            "         295,  2083, 30470,  2346,  7107,   300, 30470,   405,   724,\n",
            "        5189, 30470,   843, 17464,   899,   540, 10989,  3331,  1107,\n",
            "       30468,   119,  3221,    79,    34,  2170,    98,  9167, 30457,\n",
            "           3]), array(0), 'warung ini dimiliki oleh pengusaha pabrik tahu yang sudah puluhan tahun terkenal membuat tahu putih di bandung . tahu berkualitas , dipadu keahlian memasak , dipadu kretivitas , jadilah warung yang menyajikan menu utama berbahan tahu , ditambah menu umum lain seperti ayam . semuanya selera indonesia . harga cukup terjangkau . jangan lewatkan tahu bletoka nya , tidak kalah dengan yang asli dari tegal !')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\n",
        "print(w2i)\n",
        "print(i2w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdvOLYSRwvEy",
        "outputId": "25b5a5b4-7504-4b56-8215-4e9d27b26309"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'positive': 0, 'neutral': 1, 'negative': 2}\n",
            "{0: 'positive', 1: 'neutral', 2: 'negative'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita'\n",
        "subwords = tokenizer.encode(text)\n",
        "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
        "\n",
        "logits = model(subwords)[0]\n",
        "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
        "\n",
        "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jN9F2avPwx0v",
        "outputId": "074187d3-01c4-4020-f1c7-f9575b158035"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita | Label : positive (52.480%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning dan Evaluasi"
      ],
      "metadata": {
        "id": "ENWcFaz1xktb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=3e-6)\n",
        "model = model.cuda()"
      ],
      "metadata": {
        "id": "t9Id0g4ow1tK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "n_epochs = 5\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "\n",
        "    total_train_loss = 0\n",
        "    list_hyp, list_label = [], []\n",
        "\n",
        "    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n",
        "    for i, batch_data in enumerate(train_pbar):\n",
        "        # Forward model\n",
        "        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
        "\n",
        "        # Update model\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        tr_loss = loss.item()\n",
        "        total_train_loss = total_train_loss + tr_loss\n",
        "\n",
        "        # Calculate metrics\n",
        "        list_hyp += batch_hyp\n",
        "        list_label += batch_label\n",
        "\n",
        "        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\n",
        "            total_train_loss/(i+1), get_lr(optimizer)))\n",
        "\n",
        "    # Calculate train metric\n",
        "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
        "    print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch+1),\n",
        "        total_train_loss/(i+1), metrics_to_string(metrics), get_lr(optimizer)))\n",
        "\n",
        "    # Evaluate on validation\n",
        "    model.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "\n",
        "    total_loss, total_correct, total_labels = 0, 0, 0\n",
        "    list_hyp, list_label = [], []\n",
        "\n",
        "    pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\n",
        "    for i, batch_data in enumerate(pbar):\n",
        "        batch_seq = batch_data[-1]\n",
        "        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
        "\n",
        "        # Calculate total loss\n",
        "        valid_loss = loss.item()\n",
        "        total_loss = total_loss + valid_loss\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        list_hyp += batch_hyp\n",
        "        list_label += batch_label\n",
        "        metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
        "\n",
        "        pbar.set_description(\"VALID LOSS:{:.4f} {}\".format(total_loss/(i+1), metrics_to_string(metrics)))\n",
        "\n",
        "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
        "    print(\"(Epoch {}) VALID LOSS:{:.4f} {}\".format((epoch+1),\n",
        "        total_loss/(i+1), metrics_to_string(metrics)))"
      ],
      "metadata": {
        "id": "MHiSveKOxDZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "total_loss, total_correct, total_labels = 0, 0, 0\n",
        "list_hyp, list_label = [], []\n",
        "\n",
        "pbar = tqdm(test_loader, leave=True, total=len(test_loader))\n",
        "for i, batch_data in enumerate(pbar):\n",
        "    _, batch_hyp, _ = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
        "    list_hyp += batch_hyp\n",
        "\n",
        "# Save prediction\n",
        "df = pd.DataFrame({'label':list_hyp}).reset_index()\n",
        "df.to_csv('pred.txt', index=False)\n",
        "\n",
        "print(df)\n",
        "# Evaluate on test\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "total_loss, total_correct, total_labels = 0, 0, 0\n",
        "list_hyp, list_label = [], []\n",
        "\n",
        "pbar = tqdm(test_loader, leave=True, total=len(test_loader))\n",
        "for i, batch_data in enumerate(pbar):\n",
        "    _, batch_hyp, _ = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
        "    list_hyp += batch_hyp\n",
        "\n",
        "# Save prediction\n",
        "df = pd.DataFrame({'label':list_hyp}).reset_index()\n",
        "df.to_csv('pred.txt', index=False)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "ozw2QdeSxlwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Kamu tidak benci dia tapi suka dia'\n",
        "subwords = tokenizer.encode(text)\n",
        "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
        "\n",
        "logits = model(subwords)[0]\n",
        "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
        "\n",
        "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVYh6RJjxuPx",
        "outputId": "77590af8-49e4-4e87-a226-c83def736d1b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Kamu tidak benci dia tapi suka dia | Label : positive (99.511%)\n"
          ]
        }
      ]
    }
  ]
}